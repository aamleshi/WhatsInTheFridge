{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensornets as nets\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import pickle\n",
    "import math\n",
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Guide:  \n",
    "- [X] Read In Images  \n",
    "- [ ] Create Labels\n",
    "- [ ] Split data into training and validation- \n",
    "- [ ] Load in Model\n",
    "- [ ] Change last layer of model\n",
    "- [ ] Manipulate images to fit model\n",
    "- [ ] Train Model with new data\n",
    "- [ ] Validate Perfomance\n",
    "- [ ] Save Model\n",
    "- [ ] Write function to classify 1 image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Images From File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ced17bb49824954a15bdbff5028da49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be26cb770f14634a617575485b6dabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=56), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c0db5bde2507>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mpix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mdataDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclassID\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclassID\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classes = [\"Can\", \"Cookies\", \"Eggs\", \"Empty\", \"Fruit\"]\n",
    "dataDict = {classID: [] for classID in classes}\n",
    "img_dir = \"./Classes/\"\n",
    "for classID in tqdm_notebook(classes):\n",
    "    path = img_dir + classID\n",
    "    images = os.listdir(path)\n",
    "    #print(len(images), classID)\n",
    "    for image in tqdm_notebook(images):\n",
    "        pic = Image.open(os.path.join(path,image))\n",
    "        pix = np.array(pic.getdata()).reshape(pic.size[0], pic.size[1], 3)\n",
    "        dataDict[classID] = dataDict[classID] + [pix]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into train and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(x):\n",
    "    classEncoder = {classID:i for (i, classID) in enumerate(classes)}\n",
    "    encoded = np.zeros((len(x), 5))\n",
    "    \n",
    "    for idx, val in enumerate(x):\n",
    "        encoded[idx][classEncoder[val]] = 1\n",
    "        \n",
    "    return encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 21334.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 29\n",
      "240 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_validate_split(validfrac = .1):\n",
    "    valid_features = []\n",
    "    valid_labels = []\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    for classID in tqdm(classes):\n",
    "        num_images = len(dataDict[classID])\n",
    "        validIdx = math.ceil(num_images*validfrac)\n",
    "        valid_features.extend(dataDict[classID][:validIdx])\n",
    "        valid_labels.extend([classID]*validIdx)\n",
    "        train_features.extend(dataDict[classID][validIdx:])\n",
    "        train_labels.extend([classID]*(num_images-validIdx))\n",
    "    return (valid_features, valid_labels, train_features, train_labels)\n",
    "\n",
    "(valid_features, valid_labels, train_features, train_labels) = train_validate_split()\n",
    "one_hot_valid = one_hot_encode(valid_labels)\n",
    "one_hot_train = one_hot_encode(train_labels)\n",
    "print(len(valid_features), len(valid_labels))\n",
    "print(len(train_features), len(train_labels))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_and_save(features, labels, filename):\n",
    "    labels = one_hot_encode(labels)\n",
    "\n",
    "    pickle.dump((features, labels), open(filename, 'wb'))\n",
    "\n",
    "\n",
    "def preprocess_and_save_data():\n",
    "    n_batches = 15\n",
    "    \n",
    "    (valid_features, valid_labels, train_features, train_labels) = train_validate_split()\n",
    "    \n",
    "    batchLen = math.floor(len(valid_features)/n_batches)\n",
    "    for batch_i in range(1, n_batches + 1):\n",
    "        features = train_features[(batch_i-1)*batchLen:(batch_i*batchLen)]\n",
    "        labels = train_labels[(batch_i-1)*batchLen:(batch_i*batchLen)]\n",
    "        \n",
    "        # find index to be the point as validation data in the whole dataset of the batch (10%)\n",
    "\n",
    "        # preprocess the 90% of the whole dataset of the batch\n",
    "        # - normalize the features\n",
    "        # - one_hot_encode the lables\n",
    "        # - save in a new file named, \"preprocess_batch_\" + batch_number\n",
    "        # - each file for each batch\n",
    "        _preprocess_and_save(features, labels, \n",
    "                             'preprocess_batch_' + str(batch_i) + '.p')\n",
    "    _preprocess_and_save(np.array(valid_features), np.array(valid_labels),\n",
    "                         'preprocess_validation.p')\n",
    "    _preprocess_and_save(np.array(valid_features), np.array(valid_labels),\n",
    "                         'preprocess_testing.p')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 20400.31it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocess_and_save_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT and OUTPUT tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, 224, 224, 3), name='input_x')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 5), name='output_y')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HYPER-PARAMETERS\n",
    "learning_rate = 0.00001\n",
    "epochs = 16\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/arash/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/arash/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/arash/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/arash/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "logits = nets.VGG19(x, is_training=True, classes=5)\n",
    "model = tf.identity(logits,name='logits')\n",
    "loss = tf.losses.softmax_cross_entropy(y,logits)\n",
    "train = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(model,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scope: vgg19\n",
      "conv1/1/conv/BiasAdd:0 (?, 224, 224, 64)\n",
      "conv1/1/Relu:0 (?, 224, 224, 64)\n",
      "conv1/2/conv/BiasAdd:0 (?, 224, 224, 64)\n",
      "conv1/2/Relu:0 (?, 224, 224, 64)\n",
      "conv1/pool/MaxPool:0 (?, 112, 112, 64)\n",
      "conv2/1/conv/BiasAdd:0 (?, 112, 112, 128)\n",
      "conv2/1/Relu:0 (?, 112, 112, 128)\n",
      "conv2/2/conv/BiasAdd:0 (?, 112, 112, 128)\n",
      "conv2/2/Relu:0 (?, 112, 112, 128)\n",
      "conv2/pool/MaxPool:0 (?, 56, 56, 128)\n",
      "conv3/1/conv/BiasAdd:0 (?, 56, 56, 256)\n",
      "conv3/1/Relu:0 (?, 56, 56, 256)\n",
      "conv3/2/conv/BiasAdd:0 (?, 56, 56, 256)\n",
      "conv3/2/Relu:0 (?, 56, 56, 256)\n",
      "conv3/3/conv/BiasAdd:0 (?, 56, 56, 256)\n",
      "conv3/3/Relu:0 (?, 56, 56, 256)\n",
      "conv3/4/conv/BiasAdd:0 (?, 56, 56, 256)\n",
      "conv3/4/Relu:0 (?, 56, 56, 256)\n",
      "conv3/pool/MaxPool:0 (?, 28, 28, 256)\n",
      "conv4/1/conv/BiasAdd:0 (?, 28, 28, 512)\n",
      "conv4/1/Relu:0 (?, 28, 28, 512)\n",
      "conv4/2/conv/BiasAdd:0 (?, 28, 28, 512)\n",
      "conv4/2/Relu:0 (?, 28, 28, 512)\n",
      "conv4/3/conv/BiasAdd:0 (?, 28, 28, 512)\n",
      "conv4/3/Relu:0 (?, 28, 28, 512)\n",
      "conv4/4/conv/BiasAdd:0 (?, 28, 28, 512)\n",
      "conv4/4/Relu:0 (?, 28, 28, 512)\n",
      "conv4/pool/MaxPool:0 (?, 14, 14, 512)\n",
      "conv5/1/conv/BiasAdd:0 (?, 14, 14, 512)\n",
      "conv5/1/Relu:0 (?, 14, 14, 512)\n",
      "conv5/2/conv/BiasAdd:0 (?, 14, 14, 512)\n",
      "conv5/2/Relu:0 (?, 14, 14, 512)\n",
      "conv5/3/conv/BiasAdd:0 (?, 14, 14, 512)\n",
      "conv5/3/Relu:0 (?, 14, 14, 512)\n",
      "conv5/4/conv/BiasAdd:0 (?, 14, 14, 512)\n",
      "conv5/4/Relu:0 (?, 14, 14, 512)\n",
      "conv5/pool/MaxPool:0 (?, 7, 7, 512)\n",
      "flatten/flatten/Reshape:0 (?, 25088)\n",
      "fc6/BiasAdd:0 (?, 4096)\n",
      "relu6:0 (?, 4096)\n",
      "drop6/dropout/mul:0 (?, 4096)\n",
      "fc7/BiasAdd:0 (?, 4096)\n",
      "relu7:0 (?, 4096)\n",
      "drop7/dropout/mul:0 (?, 4096)\n",
      "logits/BiasAdd:0 (?, 5)\n",
      "probs:0 (?, 5)\n"
     ]
    }
   ],
   "source": [
    "logits.print_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scope: vgg19\n",
      "Total layers: 19\n",
      "Total weights: 114\n",
      "Total parameters: 418,772,175\n"
     ]
    }
   ],
   "source": [
    "logits.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_features_labels(features, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Split features and labels into batches\n",
    "    \"\"\"\n",
    "    for start in range(0, len(features), batch_size):\n",
    "        end = min(start + batch_size, len(features))\n",
    "        yield features[start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess_training_batch(batch_id, batch_size):\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    filename = 'preprocess_batch_' + str(batch_id) + '.p'\n",
    "    features, labels = pickle.load(open(filename, mode='rb'))\n",
    "    \n",
    "    tmpFeatures = []\n",
    "    \n",
    "    for feature in features:\n",
    "        tmpFeature = skimage.transform.resize(feature, (224, 224), mode='constant')\n",
    "        tmpFeatures.append(tmpFeature)\n",
    "\n",
    "    # Return the training data in batches of size <batch_size> or less\n",
    "    return batch_features_labels(tmpFeatures, labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arash/anaconda3/lib/python3.7/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/home/arash/anaconda3/lib/python3.7/site-packages/skimage/util/dtype.py:141: UserWarning: Possible precision loss when converting from int64 to float64\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    }
   ],
   "source": [
    "tmpValidFeatures = []\n",
    "\n",
    "for feature in valid_features:\n",
    "    tmpValidFeature = skimage.transform.resize(feature, (224, 224), mode='constant')\n",
    "    tmpValidFeatures.append(tmpValidFeature)\n",
    "    \n",
    "tmpValidFeatures = np.array(tmpValidFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(tmpValidFeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "global_variables_initializer ... done ...\n",
      "model.pretrained ... done ... \n",
      "starting training ... \n",
      "Epoch  1, Batch 1:  Validation Accuracy: 0.206897\n",
      "Epoch  1, Batch 2:  Validation Accuracy: 0.222812\n",
      "Epoch  1, Batch 3:  Validation Accuracy: 0.257294\n",
      "Epoch  1, Batch 4:  Validation Accuracy: 0.241379\n",
      "Epoch  1, Batch 5:  Validation Accuracy: 0.206897\n",
      "Epoch  2, Batch 1:  Validation Accuracy: 0.206897\n",
      "Epoch  2, Batch 2:  Validation Accuracy: 0.206897\n",
      "Epoch  2, Batch 3:  Validation Accuracy: 0.206897\n",
      "Epoch  2, Batch 4:  Validation Accuracy: 0.206897\n",
      "Epoch  2, Batch 5:  Validation Accuracy: 0.206897\n",
      "Epoch  3, Batch 1:  Validation Accuracy: 0.206897\n",
      "Epoch  3, Batch 2:  Validation Accuracy: 0.206897\n",
      "Epoch  3, Batch 3:  Validation Accuracy: 0.206897\n",
      "Epoch  3, Batch 4:  Validation Accuracy: 0.206897\n",
      "Epoch  3, Batch 5:  Validation Accuracy: 0.206897\n",
      "Epoch  4, Batch 1:  Validation Accuracy: 0.206897\n",
      "Epoch  4, Batch 2:  Validation Accuracy: 0.206897\n",
      "Epoch  4, Batch 3:  Validation Accuracy: 0.206897\n",
      "Epoch  4, Batch 4:  Validation Accuracy: 0.206897\n",
      "Epoch  4, Batch 5:  Validation Accuracy: 0.206897\n",
      "Epoch  5, Batch 1:  Validation Accuracy: 0.206897\n",
      "Epoch  5, Batch 2:  Validation Accuracy: 0.206897\n",
      "Epoch  5, Batch 3:  "
     ]
    }
   ],
   "source": [
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:    \n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('global_variables_initializer ... done ...')\n",
    "    sess.run(logits.pretrained())\n",
    "    print('model.pretrained ... done ... ')    \n",
    "    \n",
    "    # Training cycle\n",
    "    print('starting training ... ')\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):\n",
    "                sess.run(train, {x: batch_features, y: batch_labels})\n",
    "                \n",
    "            print('Epoch {:>2}, Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            \n",
    "            # calculate the mean accuracy over all validation dataset\n",
    "            valid_acc = 0\n",
    "            for batch_valid_features, batch_valid_labels in batch_features_labels(tmpValidFeatures, valid_labels, batch_size):\n",
    "                valid_acc += sess.run(accuracy, {x:batch_valid_features, y:batch_valid_labels})\n",
    "            \n",
    "            tmp_num = tmpValidFeatures.shape[0]/batch_size\n",
    "            print('Validation Accuracy: {:.6f}'.format(valid_acc/tmp_num))\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "https://github.com/deep-diver/CIFAR10-VGG19-Tensorflow/blob/master/CIFAR10-transfer-learning-tensornets.ipynb  \n",
    "https://towardsdatascience.com/transfer-learning-in-tensorflow-9e4f7eae3bb4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
