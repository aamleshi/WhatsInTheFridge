{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensornets as nets\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13682502242802756221\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16600866085959785813\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6222069963342745007\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7132662989\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2896327621005324517\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:09:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22. 28.]\n",
      " [49. 64.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.device('gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "with tf.Session() as sess:\n",
    "    print (sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no errors are thrown in above cells, GPU connection worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(path, batchsize = 32):\n",
    "    try:\n",
    "        labels = np.load(\"batchedData/\"+path+\"/labels\")\n",
    "    except:\n",
    "        print(\"No labels found\", \"batchedData/\"+path+\"/labels\")\n",
    "        return 0\n",
    "    try:\n",
    "        features = []\n",
    "        i = 0\n",
    "        while True:\n",
    "            features.append(np.load(\"batchedData/\"+path+'/'+str(i)))\n",
    "            i = i+1\n",
    "    except:\n",
    "        return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features, valid_labels = getData('preprocess_validation.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, 224, 224, 3), name='input_x')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 5), name='output_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HYPER-PARAMETERS\n",
    "learning_rate = 0.00001\n",
    "epochs = 2\n",
    "batch_size = 32\n",
    "n_batches = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = nets.VGG19(x, is_training=True, classes=5)\n",
    "model = tf.identity(logits,name='logits')\n",
    "loss = tf.losses.softmax_cross_entropy(y,logits)\n",
    "train = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(model,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scope: vgg19\n",
      "conv1/1/conv/BiasAdd:0 (?, 224, 224, 64)\n",
      "conv1/1/Relu:0 (?, 224, 224, 64)\n",
      "conv1/2/conv/BiasAdd:0 (?, 224, 224, 64)\n",
      "conv1/2/Relu:0 (?, 224, 224, 64)\n",
      "conv1/pool/MaxPool:0 (?, 112, 112, 64)\n",
      "conv2/1/conv/BiasAdd:0 (?, 112, 112, 128)\n",
      "conv2/1/Relu:0 (?, 112, 112, 128)\n",
      "conv2/2/conv/BiasAdd:0 (?, 112, 112, 128)\n",
      "conv2/2/Relu:0 (?, 112, 112, 128)\n",
      "conv2/pool/MaxPool:0 (?, 56, 56, 128)\n",
      "conv3/1/conv/BiasAdd:0 (?, 56, 56, 256)\n",
      "conv3/1/Relu:0 (?, 56, 56, 256)\n",
      "conv3/2/conv/BiasAdd:0 (?, 56, 56, 256)\n",
      "conv3/2/Relu:0 (?, 56, 56, 256)\n",
      "conv3/3/conv/BiasAdd:0 (?, 56, 56, 256)\n",
      "conv3/3/Relu:0 (?, 56, 56, 256)\n",
      "conv3/4/conv/BiasAdd:0 (?, 56, 56, 256)\n",
      "conv3/4/Relu:0 (?, 56, 56, 256)\n",
      "conv3/pool/MaxPool:0 (?, 28, 28, 256)\n",
      "conv4/1/conv/BiasAdd:0 (?, 28, 28, 512)\n",
      "conv4/1/Relu:0 (?, 28, 28, 512)\n",
      "conv4/2/conv/BiasAdd:0 (?, 28, 28, 512)\n",
      "conv4/2/Relu:0 (?, 28, 28, 512)\n",
      "conv4/3/conv/BiasAdd:0 (?, 28, 28, 512)\n",
      "conv4/3/Relu:0 (?, 28, 28, 512)\n",
      "conv4/4/conv/BiasAdd:0 (?, 28, 28, 512)\n",
      "conv4/4/Relu:0 (?, 28, 28, 512)\n",
      "conv4/pool/MaxPool:0 (?, 14, 14, 512)\n",
      "conv5/1/conv/BiasAdd:0 (?, 14, 14, 512)\n",
      "conv5/1/Relu:0 (?, 14, 14, 512)\n",
      "conv5/2/conv/BiasAdd:0 (?, 14, 14, 512)\n",
      "conv5/2/Relu:0 (?, 14, 14, 512)\n",
      "conv5/3/conv/BiasAdd:0 (?, 14, 14, 512)\n",
      "conv5/3/Relu:0 (?, 14, 14, 512)\n",
      "conv5/4/conv/BiasAdd:0 (?, 14, 14, 512)\n",
      "conv5/4/Relu:0 (?, 14, 14, 512)\n",
      "conv5/pool/MaxPool:0 (?, 7, 7, 512)\n",
      "flatten/flatten/Reshape:0 (?, 25088)\n",
      "fc6/BiasAdd:0 (?, 4096)\n",
      "relu6:0 (?, 4096)\n",
      "drop6/dropout/mul:0 (?, 4096)\n",
      "fc7/BiasAdd:0 (?, 4096)\n",
      "relu7:0 (?, 4096)\n",
      "drop7/dropout/mul:0 (?, 4096)\n",
      "logits/BiasAdd:0 (?, 5)\n",
      "probs:0 (?, 5)\n"
     ]
    }
   ],
   "source": [
    "logits.print_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scope: vgg19\n",
      "Total layers: 19\n",
      "Total weights: 114\n",
      "Total parameters: 418,772,175\n"
     ]
    }
   ],
   "source": [
    "logits.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_features_labels(features, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Split features and labels into batches\n",
    "    \"\"\"\n",
    "    for start in range(0, len(features), batch_size):\n",
    "        end = min(start + batch_size, len(features))\n",
    "        yield features[start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess_training_batch(batch_id, batch_size):\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    filename = 'preprocess_batch_' + str(batch_id) + '.p'\n",
    "    features, labels = getData(filename)\n",
    "\n",
    "    # Return the training data in batches of size <batch_size> or less\n",
    "    return batch_features_labels(features, labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "global_variables_initializer ... done ...\n",
      "model.pretrained ... done ... \n",
      "starting training ... \n",
      "Epoch  1, Batch 1:  Validation Accuracy: 0.187500\n",
      "Epoch  1, Batch 2:  Validation Accuracy: 0.208333\n",
      "Epoch  1, Batch 3:  Validation Accuracy: 0.265625\n",
      "Epoch  1, Batch 4:  Validation Accuracy: 0.322917\n",
      "Epoch  1, Batch 5:  Validation Accuracy: 0.348958\n",
      "Epoch  1, Batch 6:  Validation Accuracy: 0.364583\n",
      "Epoch  1, Batch 7:  Validation Accuracy: 0.322917\n",
      "Epoch  1, Batch 8:  Validation Accuracy: 0.343750\n",
      "Epoch  1, Batch 9:  Validation Accuracy: 0.473958\n",
      "Epoch  1, Batch 10:  Validation Accuracy: 0.463542\n",
      "Epoch  1, Batch 11:  Validation Accuracy: 0.604167\n",
      "Epoch  1, Batch 12:  Validation Accuracy: 0.661458\n",
      "Epoch  1, Batch 13:  Validation Accuracy: 0.666667\n",
      "Epoch  1, Batch 14:  Validation Accuracy: 0.708333\n",
      "Epoch  1, Batch 15:  Validation Accuracy: 0.739583\n",
      "0.7395833333333334\n",
      "Epoch  2, Batch 1:  Validation Accuracy: 0.739583\n",
      "Epoch  2, Batch 2:  Validation Accuracy: 0.812500\n",
      "Epoch  2, Batch 3:  Validation Accuracy: 0.802083\n",
      "Epoch  2, Batch 4:  Validation Accuracy: 0.786458\n",
      "Epoch  2, Batch 5:  Validation Accuracy: 0.864583\n",
      "Epoch  2, Batch 6:  Validation Accuracy: 0.859375\n",
      "Epoch  2, Batch 7:  Validation Accuracy: 0.875000\n",
      "Epoch  2, Batch 8:  Validation Accuracy: 0.885417\n",
      "Epoch  2, Batch 9:  Validation Accuracy: 0.916667\n",
      "Epoch  2, Batch 10:  Validation Accuracy: 0.916667\n",
      "Epoch  2, Batch 11:  Validation Accuracy: 0.906250\n",
      "Epoch  2, Batch 12:  Validation Accuracy: 0.911458\n",
      "Epoch  2, Batch 13:  Validation Accuracy: 0.942708\n",
      "Epoch  2, Batch 14:  Validation Accuracy: 0.937500\n",
      "Epoch  2, Batch 15:  Validation Accuracy: 0.947917\n",
      "0.9479166666666666\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "temp_n_batches =15\n",
    "with tf.Session() as sess:    \n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('global_variables_initializer ... done ...')\n",
    "    sess.run(logits.pretrained())\n",
    "    print('model.pretrained ... done ... ')    \n",
    "\n",
    "    # Training cycle\n",
    "    print('starting training ... ')\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        #n_batches = 58\n",
    "        for batch_i in range(1, temp_n_batches + 1):\n",
    "            for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):\n",
    "                sess.run(train, {x: batch_features, y: batch_labels})\n",
    "\n",
    "            print('Epoch {:>2}, Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "\n",
    "            # calculate the mean accuracy over all validation dataset\n",
    "            valid_acc = 0\n",
    "            for batch_valid_features, batch_valid_labels in batch_features_labels(valid_features, valid_labels, batch_size):\n",
    "                valid_acc += sess.run(accuracy, {x:batch_valid_features, y:batch_valid_labels})\n",
    "\n",
    "            tmp_num = len(valid_features)/batch_size\n",
    "            print('Validation Accuracy: {:.6f}'.format(valid_acc/tmp_num))\n",
    "\n",
    "        print(valid_acc/tmp_num)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:    \n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('global_variables_initializer ... done ...')\n",
    "    sess.run(logits.pretrained())\n",
    "    print('model.pretrained ... done ... ')    \n",
    "    \n",
    "    # Training cycle\n",
    "    print('starting training ... ')\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        #n_batches = 58\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):\n",
    "                sess.run(train, {x: batch_features, y: batch_labels})\n",
    "                \n",
    "            print('Epoch {:>2}, Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            \n",
    "            # calculate the mean accuracy over all validation dataset\n",
    "            valid_acc = 0\n",
    "            for batch_valid_features, batch_valid_labels in batch_features_labels(valid_features, valid_labels, batch_size):\n",
    "                valid_acc += sess.run(accuracy, {x:batch_valid_features, y:batch_valid_labels})\n",
    "            \n",
    "            tmp_num = len(valid_features)/batch_size\n",
    "            print('Validation Accuracy: {:.6f}'.format(valid_acc/tmp_num))\n",
    "            \n",
    "    # Save Model\n",
    "    print(\"saving model\")\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)\n",
    "    print(\"model saved to: \",save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_names():\n",
    "    return [\"Can\", \"Cookies\", \"Eggs\", \"Empty\", \"Fruit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def display_image_predictions(features, labels, predictions):\n",
    "    n_classes = 5\n",
    "    label_names = load_label_names()\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    label_binarizer.fit(range(n_classes))\n",
    "    label_ids = label_binarizer.inverse_transform(np.array(labels))\n",
    "\n",
    "    fig, axs = plt.subplots(10, 2, figsize=(12,24))\n",
    "\n",
    "    margin = 0.05\n",
    "    ind = np.arange(n_classes)\n",
    "    width = (1. - 2. * margin) / n_classes    \n",
    "    \n",
    "    for image_i, (feature, label_id, prediction) in enumerate(zip(features, label_ids, predictions)):\n",
    "        correct_name = label_names[label_id]\n",
    "        pred_name = label_names[np.argmax(prediction)]\n",
    "        \n",
    "        is_match = 'False'        \n",
    "        \n",
    "        if np.argmax(prediction) == label_id:\n",
    "            is_match = 'True'\n",
    "            \n",
    "        predictions_array = []\n",
    "        pred_names = []\n",
    "        \n",
    "        for index, pred_value in enumerate(prediction):\n",
    "            tmp_pred_name = label_names[index]\n",
    "            predictions_array.append({tmp_pred_name : pred_value})\n",
    "            pred_names.append(tmp_pred_name)\n",
    "        \n",
    "        print('[{}] ground truth: {}, predicted result: {} | {}'.format(image_i, correct_name, pred_name, is_match))\n",
    "        print('\\t- {}\\n'.format(predictions_array))\n",
    "        \n",
    "#         print('image_i: ', image_i)\n",
    "#         print('axs: ', axs, ', axs len: ', len(axs))\n",
    "        axs[image_i][0].imshow(feature)\n",
    "        axs[image_i][0].set_title(pred_name)\n",
    "        axs[image_i][0].set_axis_off()\n",
    "        \n",
    "        axs[image_i][1].barh(ind + margin, prediction, width)\n",
    "        axs[image_i][1].set_yticks(ind + margin)\n",
    "        axs[image_i][1].set_yticklabels(pred_names)\n",
    "        \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, test_labels = getData('preprocess_testing.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import random\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "batch_size = 32\n",
    "n_samples = 10\n",
    "top_n_predictions = 5\n",
    "\n",
    "def test_model(test_features):\n",
    "    loaded_graph = tf.Graph()\n",
    "    \n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        loaded_x = loaded_graph.get_tensor_by_name('input_x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('output_y:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        \n",
    "           \n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.softmax(loaded_logits),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels})\n",
    "        \n",
    "        display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "test_model(test_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
