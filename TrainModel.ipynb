{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensornets as nets\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(path, batchsize = 16):\n",
    "    try:\n",
    "        labels = np.load(\"batchedData/\"+path+\"/labels\")\n",
    "    except:\n",
    "        print(\"No labels found\", \"batchedData\"+path+\"/labels\")\n",
    "        return 0\n",
    "    try:\n",
    "        features = []\n",
    "        i = 0\n",
    "        while True:\n",
    "            features.append(np.load(path+'/'+str(i)))\n",
    "            i = i+1\n",
    "    except:\n",
    "        return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features, valid_labels = getData('preprocess_validation.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, 224, 224, 3), name='input_x')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 5), name='output_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HYPER-PARAMETERS\n",
    "learning_rate = 0.00001\n",
    "epochs = 6\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/arash/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/arash/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/arash/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/arash/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "logits = nets.VGG19(x, is_training=True, classes=5)\n",
    "model = tf.identity(logits,name='logits')\n",
    "loss = tf.losses.softmax_cross_entropy(y,logits)\n",
    "train = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(model,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scope: vgg19\n",
      "conv1/1/conv/BiasAdd:0 (?, 224, 224, 64)\n",
      "conv1/1/Relu:0 (?, 224, 224, 64)\n",
      "conv1/2/conv/BiasAdd:0 (?, 224, 224, 64)\n",
      "conv1/2/Relu:0 (?, 224, 224, 64)\n",
      "conv1/pool/MaxPool:0 (?, 112, 112, 64)\n",
      "conv2/1/conv/BiasAdd:0 (?, 112, 112, 128)\n",
      "conv2/1/Relu:0 (?, 112, 112, 128)\n",
      "conv2/2/conv/BiasAdd:0 (?, 112, 112, 128)\n",
      "conv2/2/Relu:0 (?, 112, 112, 128)\n",
      "conv2/pool/MaxPool:0 (?, 56, 56, 128)\n",
      "conv3/1/conv/BiasAdd:0 (?, 56, 56, 256)\n",
      "conv3/1/Relu:0 (?, 56, 56, 256)\n",
      "conv3/2/conv/BiasAdd:0 (?, 56, 56, 256)\n",
      "conv3/2/Relu:0 (?, 56, 56, 256)\n",
      "conv3/3/conv/BiasAdd:0 (?, 56, 56, 256)\n",
      "conv3/3/Relu:0 (?, 56, 56, 256)\n",
      "conv3/4/conv/BiasAdd:0 (?, 56, 56, 256)\n",
      "conv3/4/Relu:0 (?, 56, 56, 256)\n",
      "conv3/pool/MaxPool:0 (?, 28, 28, 256)\n",
      "conv4/1/conv/BiasAdd:0 (?, 28, 28, 512)\n",
      "conv4/1/Relu:0 (?, 28, 28, 512)\n",
      "conv4/2/conv/BiasAdd:0 (?, 28, 28, 512)\n",
      "conv4/2/Relu:0 (?, 28, 28, 512)\n",
      "conv4/3/conv/BiasAdd:0 (?, 28, 28, 512)\n",
      "conv4/3/Relu:0 (?, 28, 28, 512)\n",
      "conv4/4/conv/BiasAdd:0 (?, 28, 28, 512)\n",
      "conv4/4/Relu:0 (?, 28, 28, 512)\n",
      "conv4/pool/MaxPool:0 (?, 14, 14, 512)\n",
      "conv5/1/conv/BiasAdd:0 (?, 14, 14, 512)\n",
      "conv5/1/Relu:0 (?, 14, 14, 512)\n",
      "conv5/2/conv/BiasAdd:0 (?, 14, 14, 512)\n",
      "conv5/2/Relu:0 (?, 14, 14, 512)\n",
      "conv5/3/conv/BiasAdd:0 (?, 14, 14, 512)\n",
      "conv5/3/Relu:0 (?, 14, 14, 512)\n",
      "conv5/4/conv/BiasAdd:0 (?, 14, 14, 512)\n",
      "conv5/4/Relu:0 (?, 14, 14, 512)\n",
      "conv5/pool/MaxPool:0 (?, 7, 7, 512)\n",
      "flatten/flatten/Reshape:0 (?, 25088)\n",
      "fc6/BiasAdd:0 (?, 4096)\n",
      "relu6:0 (?, 4096)\n",
      "drop6/dropout/mul:0 (?, 4096)\n",
      "fc7/BiasAdd:0 (?, 4096)\n",
      "relu7:0 (?, 4096)\n",
      "drop7/dropout/mul:0 (?, 4096)\n",
      "logits/BiasAdd:0 (?, 5)\n",
      "probs:0 (?, 5)\n"
     ]
    }
   ],
   "source": [
    "logits.print_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scope: vgg19\n",
      "Total layers: 19\n",
      "Total weights: 114\n",
      "Total parameters: 418,772,175\n"
     ]
    }
   ],
   "source": [
    "logits.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_features_labels(features, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Split features and labels into batches\n",
    "    \"\"\"\n",
    "    for start in range(0, len(features), batch_size):\n",
    "        end = min(start + batch_size, len(features))\n",
    "        yield features[start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess_training_batch(batch_id, batch_size):\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    filename = 'preprocess_batch_' + str(batch_id) + '.p'\n",
    "    features, labels = getData(filename)\n",
    "\n",
    "    # Return the training data in batches of size <batch_size> or less\n",
    "    return batch_features_labels(features, labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9a018a817466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Initializing the variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:    \n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('global_variables_initializer ... done ...')\n",
    "    sess.run(logits.pretrained())\n",
    "    print('model.pretrained ... done ... ')    \n",
    "    \n",
    "    # Training cycle\n",
    "    print('starting training ... ')\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 11\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):\n",
    "                sess.run(train, {x: batch_features, y: batch_labels})\n",
    "                \n",
    "            print('Epoch {:>2}, Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            \n",
    "            # calculate the mean accuracy over all validation dataset\n",
    "            valid_acc = 0\n",
    "            for batch_valid_features, batch_valid_labels in batch_features_labels(valid_features, valid_labels, batch_size):\n",
    "                valid_acc += sess.run(accuracy, {x:batch_valid_features, y:batch_valid_labels})\n",
    "            \n",
    "            tmp_num = tmpValidFeatures.shape[0]/batch_size\n",
    "            print('Validation Accuracy: {:.6f}'.format(valid_acc/tmp_num))\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
